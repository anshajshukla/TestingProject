"""
Integration tests for ML components in the banking test suite.
This demonstrates how the ML data generator and test prioritizer work together.
"""
import pytest
import os
import pandas as pd
# Fix matplotlib backend - use non-interactive Agg backend
import matplotlib
matplotlib.use('Agg')  # Must be before importing pyplot
import matplotlib.pyplot as plt
from utils.ml.data_generator import BankingDataGenerator
from utils.ml.test_prioritizer import MLTestPrioritizer

@pytest.fixture(scope="module")
def ensure_data_dirs():
    """Ensure data directories exist."""
    os.makedirs("data", exist_ok=True)
    os.makedirs("reports", exist_ok=True)

def test_banking_data_generation(ensure_data_dirs):
    """Test the ML-powered banking data generator."""
    # Create a data generator with a small number of accounts
    generator = BankingDataGenerator(num_accounts=3, seed=42)
    
    # Generate and save test data
    test_data = generator.save_test_data('data/test_banking_data.json')
    
    # Verify the generated data
    assert 'accounts' in test_data
    assert 'transactions' in test_data
    assert 'metadata' in test_data
    
    # Verify we have the expected number of accounts
    assert len(test_data['accounts']) == 3
    
    # Check that we have transactions
    assert len(test_data['transactions']) > 0
    
    # Check for anomalies
    anomalies = [t for t in test_data['transactions'] if t.get('is_anomaly', False)]
    print(f"\nGenerated {len(test_data['transactions'])} transactions, including {len(anomalies)} anomalies")
    
    # Create a visualization of transaction amounts
    amounts = [t['amount'] for t in test_data['transactions']]
    categories = [t['category'] for t in test_data['transactions']]
    
    # Create a scatter plot of transaction amounts by category
    plt.figure(figsize=(12, 6))
    
    # Plot normal transactions
    normal_amounts = [t['amount'] for t in test_data['transactions'] if not t.get('is_anomaly', False)]
    normal_categories = [t['category'] for t in test_data['transactions'] if not t.get('is_anomaly', False)]
    
    # Get unique categories
    unique_categories = list(set(normal_categories))
    
    # Create a color map
    category_colors = {}
    for i, category in enumerate(unique_categories):
        category_colors[category] = plt.cm.tab10(i % 10)
    
    # Plot normal transactions
    for category in unique_categories:
        category_amounts = [normal_amounts[i] for i, cat in enumerate(normal_categories) if cat == category]
        category_indices = [i for i, cat in enumerate(normal_categories) if cat == category]
        plt.scatter(category_indices, category_amounts, 
                   color=category_colors[category], label=category, alpha=0.7)
    
    # Plot anomalies with red stars
    if anomalies:
        anomaly_amounts = [t['amount'] for t in anomalies]
        anomaly_indices = [i for i, t in enumerate(test_data['transactions']) if t.get('is_anomaly', False)]
        plt.scatter(anomaly_indices, anomaly_amounts, color='red', marker='*', s=100, label='Anomaly')
    
    plt.title('Banking Transactions Generated by ML')
    plt.xlabel('Transaction Index')
    plt.ylabel('Amount ($)')
    plt.legend(loc='upper right')
    plt.grid(True, alpha=0.3)
    
    # Save the plot
    plt.savefig('reports/generated_transactions.png')
    print("Generated transaction visualization saved to reports/generated_transactions.png")
    
    # Basic anomaly detection - flag unusually large transactions
    q75, q25 = pd.Series(amounts).quantile([0.75, 0.25])
    iqr = q75 - q25
    upper_bound = q75 + 1.5 * iqr
    
    detected_anomalies = [i for i, amount in enumerate(amounts) if amount > upper_bound]
    print(f"Detected {len(detected_anomalies)} potential anomalies based on transaction amount")

def test_test_prioritization(ensure_data_dirs):
    """Test the ML-powered test prioritizer."""
    # Create some sample test history
    sample_history = []
    
    # Create a list of test names
    test_names = [
        'test_login', 'test_invalid_login', 'test_transfer', 
        'test_account_details', 'test_api_auth', 'test_health'
    ]
    
    # Create a list of modules
    modules = ['ui', 'api', 'smoke']
    
    # Generate sample history - intentionally make some tests fail more often
    import random
    import datetime
    random.seed(42)
    
    for i in range(100):
        test_name = random.choice(test_names)
        module = random.choice(modules)
        
        # Make certain tests fail more often
        if test_name == 'test_transfer' and module == 'ui':
            result = random.choices(['pass', 'fail'], weights=[0.6, 0.4])[0]
        elif test_name == 'test_api_auth' and module == 'api':
            result = random.choices(['pass', 'fail'], weights=[0.7, 0.3])[0]
        else:
            result = random.choices(['pass', 'fail'], weights=[0.9, 0.1])[0]
        
        duration = random.uniform(0.1, 5.0)
        timestamp = (datetime.datetime.now() - datetime.timedelta(days=random.randint(0, 30))).isoformat()
        
        sample_history.append({
            'test_name': test_name,
            'module': module,
            'result': result,
            'duration': duration,
            'timestamp': timestamp
        })
    
    # Save the sample history
    history_df = pd.DataFrame(sample_history)
    history_df.to_csv("data/sample_test_history.csv", index=False)
    
    # Create a test prioritizer
    prioritizer = MLTestPrioritizer(history_file="data/sample_test_history.csv")
    
    # Convert history DataFrame columns to string type to avoid scikit-learn feature names warning
    history_df = pd.read_csv("data/sample_test_history.csv")
    for col in history_df.columns:
        if col not in ['duration', 'timestamp']:
            history_df[col] = history_df[col].astype(str)
    
    # Train the model with processed DataFrame
    trained = prioritizer.train(history_df)
    assert trained, "Model should train successfully"
    
    # Create a list of tests to prioritize
    tests_to_run = [
        'tests/ui/test_login.py::test_login',
        'tests/ui/test_login.py::test_invalid_login',
        'tests/ui/test_transfer.py::test_transfer',
        'tests/api/test_auth.py::test_api_auth',
        'tests/smoke/test_health.py::test_health'
    ]
    
    # Prioritize the tests
    prioritized_tests = prioritizer.prioritize_tests(tests_to_run)
    
    # Print the prioritized tests
    print("\nML-Based Test Prioritization Results:")
    for i, test in enumerate(prioritized_tests):
        print(f"{i+1}. {test}")
    
    # Verify we got back the same number of tests
    assert len(prioritized_tests) == len(tests_to_run)
    
    # Analyze the prioritization - tests with higher failure rates should be prioritized
    # The 'test_transfer' and 'test_api_auth' tests should be near the top
    transfer_position = next((i for i, test in enumerate(prioritized_tests) 
                             if 'test_transfer.py::test_transfer' in test), -1)
    api_auth_position = next((i for i, test in enumerate(prioritized_tests) 
                             if 'test_auth.py::test_api_auth' in test), -1)
    
    print(f"\nTransfer test position: {transfer_position + 1}")
    print(f"API Auth test position: {api_auth_position + 1}")
    
    # These tests should be in the top 3
    assert transfer_position < 3 or api_auth_position < 3, "At least one high-failure test should be in top 3"
    
    # Create a visualization of the test failure rates
    failure_rates = []
    for test in tests_to_run:
        # Extract test name and module
        if '::' in test:
            parts = test.split('::')
            test_name = parts[-1]
            module = parts[0].split('/')[-2] if '/' in parts[0] else 'unknown'
        else:
            test_name = test
            module = 'unknown'
        
        # Calculate failure rate from history
        test_history = history_df[(history_df['test_name'] == test_name) & 
                                (history_df['module'] == module)]
        
        if not test_history.empty:
            failure_rate = (test_history['result'] == 'fail').mean()
        else:
            failure_rate = 0
        
        failure_rates.append((test, failure_rate))
    
    # Sort by failure rate
    failure_rates.sort(key=lambda x: x[1], reverse=True)
    
    # Create bar chart
    plt.figure(figsize=(12, 6))
    tests = [t[0].split('::')[-1] for t in failure_rates]
    rates = [t[1] for t in failure_rates]
    
    plt.bar(tests, rates, color='skyblue')
    plt.title('Test Failure Rates')
    plt.xlabel('Test')
    plt.ylabel('Failure Rate')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    
    # Save the plot
    plt.savefig('reports/test_failure_rates.png')
    print("Test failure rates visualization saved to reports/test_failure_rates.png")

if __name__ == "__main__":
    # Run the tests
    test_banking_data_generation(None)
    test_test_prioritization(None)
